[
  {"id":"blumFoundationsDataScience","author":[{"family":"Blum","given":"Avrim"},{"family":"Hopcroft","given":"John"},{"family":"Kannan","given":"Ravindran"}],"citation-key":"blumFoundationsDataScience","language":"en","source":"Zotero","title":"Foundations of Data Science","type":"article-journal"},
  {"id":"borgwardtGraphKernelsStateoftheArt2020","abstract":"Graph-structured data are an integral part of many application domains, including chemoinformatics, computational biology, neuroimaging, and social network analysis. Over the last two decades, numerous graph kernels, i.e. kernel functions between graphs, have been proposed to solve the problem of assessing the similarity between graphs, thereby making it possible to perform predictions in both classification and regression settings. This manuscript provides a review of existing graph kernels, their applications, software plus data resources, and an empirical comparison of state-of-the-art graph kernels.","accessed":{"date-parts":[[2022,12,18]]},"author":[{"family":"Borgwardt","given":"Karsten"},{"family":"Ghisu","given":"Elisabetta"},{"family":"Llinares-López","given":"Felipe"},{"family":"O'Bray","given":"Leslie"},{"family":"Rieck","given":"Bastian"}],"citation-key":"borgwardtGraphKernelsStateoftheArt2020","container-title":"Foundations and Trends® in Machine Learning","container-title-short":"FNT in Machine Learning","DOI":"10.1561/2200000076","ISSN":"1935-8237, 1935-8245","issue":"5-6","issued":{"date-parts":[[2020]]},"page":"531-712","source":"arXiv.org","title":"Graph Kernels: State-of-the-Art and Future Challenges","title-short":"Graph Kernels","type":"article-journal","URL":"http://arxiv.org/abs/2011.03854","volume":"13"},
  {"id":"ghojoghReproducingKernelHilbert2021","abstract":"This is a tutorial and survey paper on kernels, kernel methods, and related fields. We start with reviewing the history of kernels in functional analysis and machine learning. Then, Mercer kernel, Hilbert and Banach spaces, Reproducing Kernel Hilbert Space (RKHS), Mercer's theorem and its proof, frequently used kernels, kernel construction from distance metric, important classes of kernels (including bounded, integrally positive definite, universal, stationary, and characteristic kernels), kernel centering and normalization, and eigenfunctions are explained in detail. Then, we introduce types of use of kernels in machine learning including kernel methods (such as kernel support vector machines), kernel learning by semi-definite programming, Hilbert-Schmidt independence criterion, maximum mean discrepancy, kernel mean embedding, and kernel dimensionality reduction. We also cover rank and factorization of kernel matrix as well as the approximation of eigenfunctions and kernels using the Nystr{\\\"o}m method. This paper can be useful for various fields of science including machine learning, dimensionality reduction, functional analysis in mathematics, and mathematical physics in quantum mechanics.","accessed":{"date-parts":[[2023,1,10]]},"author":[{"family":"Ghojogh","given":"Benyamin"},{"family":"Ghodsi","given":"Ali"},{"family":"Karray","given":"Fakhri"},{"family":"Crowley","given":"Mark"}],"citation-key":"ghojoghReproducingKernelHilbert2021","issued":{"date-parts":[[2021,6,15]]},"number":"arXiv:2106.08443","publisher":"arXiv","source":"arXiv.org","title":"Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr\\\"om Method, and Use of Kernels in Machine Learning: Tutorial and Survey","title-short":"Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nystr\\\"om Method, and Use of Kernels in Machine Learning","type":"article","URL":"http://arxiv.org/abs/2106.08443"},
  {"id":"huStrategiesPretrainingGraph2020","abstract":"Many domains in machine learning have datasets with a large number of related but different tasks. Those domains are challenging because task-specific labels are often scarce and test examples can be distributionally different from examples seen during training. An effective solution to these challenges is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective for improving many language and vision domains, pre-training on graph datasets remains an open question. Here, we develop a strategy for pre-training Graph Neural Networks (GNNs). Crucial to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs. We systematically study different pre-training strategies on multiple datasets and find that when ad-hoc strategies are applied, pre-trained GNNs often exhibit negative transfer and perform worse than non-pre-trained GNNs on many downstream tasks. In contrast, our proposed strategy is effective and avoids negative transfer across downstream tasks, leading up to 11.7% absolute improvements in ROC-AUC over non-pre-trained models and achieving state of the art performance.","author":[{"family":"Hu","given":"Weihua"},{"family":"Liu","given":"Bowen"},{"family":"Gomes","given":"Joseph"},{"literal":"Joseph Gomes"},{"family":"Zitnik","given":"Marinka"},{"family":"Liang","given":"Percy"},{"family":"Pande","given":"Vijay S."},{"family":"Leskovec","given":"Jure"}],"citation-key":"huStrategiesPretrainingGraph2020","container-title":"International Conference on Learning Representations","issued":{"date-parts":[[2020,4,30]]},"note":"ARXIV_ID: 1905.12265\nMAG ID: 2996604169\nS2ID: 789a7069d1a2d02d784e4821685b216cc63e6ec8","title":"Strategies for Pre-training Graph Neural Networks","type":"article-journal"},
  {"id":"kriegeSurveyGraphKernels2020","abstract":"Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification.","accessed":{"date-parts":[[2022,12,18]]},"author":[{"family":"Kriege","given":"Nils M."},{"family":"Johansson","given":"Fredrik D."},{"family":"Morris","given":"Christopher"}],"citation-key":"kriegeSurveyGraphKernels2020","container-title":"Applied Network Science","container-title-short":"Appl Netw Sci","DOI":"10.1007/s41109-019-0195-3","ISSN":"2364-8228","issue":"1","issued":{"date-parts":[[2020,12]]},"page":"6","source":"arXiv.org","title":"A Survey on Graph Kernels","type":"article-journal","URL":"http://arxiv.org/abs/1903.11835","volume":"5"},
  {"id":"MehryarMohriFoundations","accessed":{"date-parts":[[2023,1,10]]},"citation-key":"MehryarMohriFoundations","title":"Mehryar Mohri -- Foundations of Machine Learning - Book","type":"webpage","URL":"https://cs.nyu.edu/~mohri/mlbook/"},
  {"id":"mohriFoundationsMachineLearning","author":[{"family":"Mohri","given":"Mehryar"},{"family":"Rostamizadeh","given":"Afshin"},{"family":"Talwalkar","given":"Ameet"}],"citation-key":"mohriFoundationsMachineLearning","language":"en","source":"Zotero","title":"Foundations of Machine Learning (second edition)","type":"article-journal"},
  {"id":"renQuery2boxReasoningKnowledge2020","abstract":"Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\\wedge$) and existential quantifiers ($\\exists$). Handling queries with logical disjunctions ($\\vee$) remains an open problem. Here we propose query2box, an embedding-based framework for reasoning over arbitrary queries with $\\wedge$, $\\vee$, and $\\exists$ operators in massive and incomplete KGs. Our main insight is that queries can be embedded as boxes (i.e., hyper-rectangles), where a set of points inside the box corresponds to a set of answer entities of the query. We show that conjunctions can be naturally represented as intersections of boxes and also prove a negative result that handling disjunctions would require embedding with dimension proportional to the number of KG entities. However, we show that by transforming queries into a Disjunctive Normal Form, query2box is capable of handling arbitrary logical queries with $\\wedge$, $\\vee$, $\\exists$ in a scalable manner. We demonstrate the effectiveness of query2box on two large KGs and show that query2box achieves up to 25% relative improvement over the state of the art.","author":[{"family":"Ren","given":"Hongyu"},{"family":"Hu","given":"Weihua"},{"family":"Leskovec","given":"Jure"}],"citation-key":"renQuery2boxReasoningKnowledge2020","container-title":"International Conference on Learning Representations","issued":{"date-parts":[[2020,4,30]]},"note":"ARXIV_ID: 2002.05969\nMAG ID: 2994695355\nS2ID: 66f7823be7a0adcd8ed3ecebf00e53daf45bc6bc","title":"Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings","type":"article-journal"},
  {"id":"yingsuMICOMultialternativeContrastive2022","abstract":"Commonsense reasoning tasks such as commonsense knowledge graph completion and commonsense question answering require powerful representation learning. In this paper, we propose to learn commonsense knowledge representation by MICO, a M ulti-alternative contrast I ve learning framework on CO mmonsense knowledge graphs (MICO). MICO generates the commonsense knowledge representation by contextual interaction between entity nodes and relations with multi-alternative contrastive learning. In MICO, the head and tail entities in an ( h, r, t ) knowledge triple are converted to two relation-aware sequence pairs (a premise and an alternative) in the form of natural language. Semantic representations generated by MICO can beneﬁt the following two tasks by simply comparing the distance score between the representations: Extensive experiments show the effectiveness our method.","author":[{"literal":"Ying Su"},{"literal":"Zihao Wang"},{"literal":"Tianqing Fang"},{"literal":"Hongming Zhang"},{"literal":"Yangqiu Song"},{"literal":"T. Zhang"}],"citation-key":"yingsuMICOMultialternativeContrastive2022","container-title":"ArXiv","DOI":"10.48550/arxiv.2210.07570","issued":{"date-parts":[[2022]]},"note":"ARXIV_ID: 2210.07570\nS2ID: 480e667c50eb71ba78bfde47e4686ca7b21148bd","title":"MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation","type":"article-journal"}
]
